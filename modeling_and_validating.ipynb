{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T06:28:18.579554Z",
     "start_time": "2019-05-26T06:28:16.502688Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk import bigrams\n",
    "import itertools\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import nltk\n",
    "import cld2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - method one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T06:28:19.203880Z",
     "start_time": "2019-05-26T06:28:19.166830Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"WIKI2019_2.txt\") as f:\n",
    "    all_sents = [line for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, i in enumerate(all_sents[:20000]):\n",
    "    with open(\"./wiki_txt_training/file{}.txt\".format(n),\"a+\") as f:\n",
    "        f.write(i)\n",
    "for n, i in enumerate(all_sents[20000:]):\n",
    "    with open(\"./wiki_txt_testing/file{}.txt\".format(n),\"a+\") as f:\n",
    "        f.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:37:49.018832Z",
     "start_time": "2019-05-16T00:37:48.363538Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = './wiki_txt_training/'\n",
    "wolof_corpus = PlaintextCorpusReader(corpus_root,'.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:37:58.010110Z",
     "start_time": "2019-05-16T00:37:55.554968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Albert', 'Einstein', 'juddoo', 'Ulma', 'Almaañ', 'ci', 'atum'], ['faatoo', 'Princeton', 'Diiwaan', 'yu', 'Bennoo', 'yu', 'Amerig', 'ci', 'doonoon', 'na', 'ab', 'boroom', 'xamxamu', 'jëmm'], ...]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wolof_corpus.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:39:29.571807Z",
     "start_time": "2019-05-16T00:39:16.816010Z"
    }
   },
   "outputs": [],
   "source": [
    "wolof_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    " \n",
    "for sentence in wolof_corpus.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        wolof_model[(w1, w2)][w3] += 1\n",
    "\n",
    "# transform the counts to probabilities\n",
    "for w1_w2 in wolof_model:\n",
    "    total_count = sum(wolof_model[w1_w2].values())\n",
    "    for w3 in wolof_model[w1_w2]:\n",
    "        wolof_model[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:39:40.194156Z",
     "start_time": "2019-05-16T00:39:40.182584Z"
    }
   },
   "outputs": [],
   "source": [
    "# get some example\n",
    "# sentences generated by the model\n",
    "ex = []\n",
    "for i in range(10):\n",
    "    text = [None, None]\n",
    "    sentence_finished = False\n",
    "    while not sentence_finished:\n",
    "        r = random.random()\n",
    "        accumulator = .0\n",
    "        for word in wolof_model[tuple(text[-2:])].keys():\n",
    "            accumulator += wolof_model[tuple(text[-2:])][word]\n",
    "            if accumulator >= r:\n",
    "                text.append(word)\n",
    "                break\n",
    "        if text[-2:]==[None, None]:\n",
    "            sentence_finished = True\n",
    "    alltext = ' '.join([t for t in text if t])\n",
    "    ex.append(alltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:40:10.596196Z",
     "start_time": "2019-05-16T00:40:10.582089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mu sos ngir dëkksi ci taax yi ak ñu dul ñoom',\n",
       " 'sóobu na ci yooyu',\n",
       " 'ànd ak ñoom te nu gën koo jëme kanam',\n",
       " 'amoon tamit ag joor ak ab coonaam fa ñatti at yu mujj yi daa lijjanti aka nos diggante jaam bi nekk ci sancoom yi nekk ci digg yu yaxantu',\n",
       " 'moom mi leen may ngeen àgg fa njëkk a taxaw ci di leen tegi coona',\n",
       " 'da leen daa gàddaayloo',\n",
       " 'Moom mbootaay gi nag mi ngi ci digg Afrig atum 1876g',\n",
       " 'Sàmm ag jur',\n",
       " 'don te mengo tul ak',\n",
       " 'ña ca gëm ñu ag ragal manu def la ca Baabul Mandab ak barsaqu Suwes']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:46:53.104168Z",
     "start_time": "2019-05-16T00:46:53.101267Z"
    }
   },
   "outputs": [],
   "source": [
    "# guess the third words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:46:12.666774Z",
     "start_time": "2019-05-16T00:46:12.643437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['daa', 'a', 'na', 'di'])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wolof_model['da', 'leen'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:47:49.287489Z",
     "start_time": "2019-05-16T00:47:49.284712Z"
    }
   },
   "outputs": [],
   "source": [
    "# use nltk.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:48:10.490306Z",
     "start_time": "2019-05-16T00:48:10.486458Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:50:14.390233Z",
     "start_time": "2019-05-16T00:50:14.298119Z"
    }
   },
   "outputs": [],
   "source": [
    "# from github repo WOL.txt\n",
    "with open(\"WOL.txt\") as f:\n",
    "    wolof = f.read().splitlines() \n",
    "\n",
    "with open(\"wo_SN-web_crawled.txt\") as f:\n",
    "    wol_web = f.read().splitlines() \n",
    "\n",
    "with open(\"WIKI2019_2.txt\") as f:\n",
    "    mywiki = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:50:58.944034Z",
     "start_time": "2019-05-16T00:50:58.939647Z"
    }
   },
   "outputs": [],
   "source": [
    "wolof1 = wolof + mywiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:50:59.442770Z",
     "start_time": "2019-05-16T00:50:59.434203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58768"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = wolof5\n",
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:51:00.032735Z",
     "start_time": "2019-05-16T00:51:00.022686Z"
    }
   },
   "outputs": [],
   "source": [
    "# corpus = ''\n",
    "# for i in sen:\n",
    "#     corpus = corpus + str(i) +'. '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:51:00.438934Z",
     "start_time": "2019-05-16T00:51:00.417281Z"
    }
   },
   "outputs": [],
   "source": [
    "training = sent[:int(len(sent)*0.8)]\n",
    "testing = sent[int(len(sent)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:51:01.603753Z",
     "start_time": "2019-05-16T00:51:00.603238Z"
    }
   },
   "outputs": [],
   "source": [
    "training_text = []\n",
    "for item in training:\n",
    "    training_text.append(item.split())\n",
    "\n",
    "testing_text = []\n",
    "for item in testing:\n",
    "    testing_text.append(item.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:51:01.610487Z",
     "start_time": "2019-05-16T00:51:01.606227Z"
    }
   },
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(2, training_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:51:11.911142Z",
     "start_time": "2019-05-16T00:51:01.612425Z"
    }
   },
   "outputs": [],
   "source": [
    "lm = MLE(2)\n",
    "lm.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create bigrams for testing purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:51:12.036003Z",
     "start_time": "2019-05-16T00:51:11.914629Z"
    }
   },
   "outputs": [],
   "source": [
    "test_grams = []\n",
    "for item in testing_text:\n",
    "    test_grams.append(list(bigrams(' '.join(item).split()))) \n",
    "merged_test_grams = list(itertools.chain(*test_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:51:12.724397Z",
     "start_time": "2019-05-16T00:51:12.038925Z"
    }
   },
   "outputs": [],
   "source": [
    "train_grams = []\n",
    "for item in training_text:\n",
    "    train_grams.append(list(bigrams(' '.join(item).split()))) \n",
    "merged_train_grams = list(itertools.chain(*train_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:51:12.730483Z",
     "start_time": "2019-05-16T00:51:12.726229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696127\n",
      "112457\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_train_grams))\n",
    "print(len(merged_test_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:51:12.996434Z",
     "start_time": "2019-05-16T00:51:12.733420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199534\n",
      "65647\n"
     ]
    }
   ],
   "source": [
    "train_gram_set = set(merged_train_grams)\n",
    "test_gram_set = set(merged_test_grams)\n",
    "print(len(train_gram_set))\n",
    "print(len(test_gram_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:51:13.046046Z",
     "start_time": "2019-05-16T00:51:12.998887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51680\n"
     ]
    }
   ],
   "source": [
    "intersection = train_gram_set.intersection(test_gram_set)\n",
    "print(len(intersection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:51:13.489962Z",
     "start_time": "2019-05-16T00:51:13.048901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.22471300613424"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.perplexity(list(intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
